%\section{Правила дифференцирования вещественных скалярных функций по комплексным матричным параметрам}
%
%Пусть $D_{\bit{z}}(\cdot)$~--- оператор дифференцирования по параметрам модели $\bit{z}\in\mathbb{C}^{K\times 1}$, где $K$~---~количество параметров всей модели:
%\begin{equation}
%	D_{\bit{z}}(\cdot)=\begin{pmatrix}
	%		\frac{\partial }{\partial z_0} &
	%		\frac{\partial }{\partial z_1} &
	%		\cdots &
	%		\frac{\partial }{\partial z_{K-1}}
	%	\end{pmatrix}
%\end{equation}
%Тогда $D_{\bit{z}}J\in\mathbb{C}^{1\times K}$ -- производная скалярной вещественнозначной функции $J\in\mathbb{R}$ по вектору параметров всей модели. Таким образом, дифференциал первого порядка функции $J~=~J(\bit{z}, \bit{z}^*)$ может быть представлен как~\cite{complex_deriv}:
%\begin{equation}
%	dJ=D_{\bit{z}}Jd\bit{z}+D_{\bit{z}^*}Jd\bit{z}^*,
%	\label{first_diff}
%\end{equation}
%где $D_{\bit{z}}J\in\mathbb{C}^{1\times K}$, $D_{\bit{z}^*}J\in\mathbb{C}^{1\times K}$, $d\bit{z}\in\mathbb{C}^{K\times 1}$, $d\bit{z}^*\in\mathbb{C}^{K\times 1}$.
%
%Производная второго порядка скалярной вещественной функции $J$ по вектору параметров описывается следующим образом:
%
%\begin{equation}
%	D_{\bit{z}}(D_{\bit{z}}J)^T=H_{\bit{z}, \bit{z}}J\in\mathbb{C}^{K\times K}\text{-- матрица Гёссе},
%	\label{hessian_calc}
%\end{equation}
%где $(\cdot)^T$~--- оператор транспонирования.
%
%Используя введённые выше обозначения, дифференциал второго порядка функции $J$ может быть описан следующим образом \cite{complex_deriv}:
%\begin{equation}
%	d^2J=\begin{pmatrix}
	%		d\bit{z}^T & d(\bit{z}^*)^T
	%	\end{pmatrix}
%	\begin{pmatrix}
	%		H_{\bit{z}, \bit{z}}J & H_{\bit{z}^*, \bit{z}}J \\
	%		H_{\bit{z}, \bit{z}^*}J & H_{\bit{z}^*, \bit{z}^*}J
	%	\end{pmatrix}
%	\begin{pmatrix}
	%		d\bit{z} \\ d\bit{z}^*
	%	\end{pmatrix}
%\end{equation}

\subsection{Критерий компенсации нелинейных искажений в устройствах связи}

Существует несколько способов обработки сигнала \cite{adapt_filt_haykin}. Cимуляция алгоритмов как правило проводится на сохраненных блоках данных. В этом случае имеется мощный вычислитель (компьютер) и возможность вести математическую обработку матриц и других объектов алгоритмов. Это блочный режим работы.

При аппаратной реализации блочный режим стараются заменить стохастическим в реальном времени \cite{adapt_filt_haykin}, чтобы минимизировать задержки и сократить ресурсы. В этом случае векторы состояния, корреляционные матрицы, шаг алгоритма, а также другие объекты, обновляются каждый отсчет.

При рассмотрении методов компенсации паразитных помех будем считать, что на вход блока адаптации поступает вектор комплексных отсчетов передатчика $\textbf{\textit{x}}\in\mathbb{C}^{N\times1}$ длины $N$, вектор отсчетов с приёмника $\textbf{\textit{d}}\in\mathbb{C}^{N\times1}$ также длины $N$.

Задача блока адаптации заключается в том, чтобы так преобразовать исходный вектор $\textbf{\textit{x}}$, чтобы как можно лучше приблизить к вектору отсчетов на приёмнике $\textbf{\textit{d}}$ путём минимизации нормы вектора ошибки $\textbf{\textit{e}}=\textbf{\textit{d}}-f(\textbf{\textit{x}}, \bit{z})\in\mathbb{C}^{N\times1}$, где $f(\cdot)$ -- оператор преобразования исходного вектора в вектор отсчетов на выходе модели помехи. Оператор $f(\cdot)$ определяется коэфициентами $\textbf{\textit{z}}\in\mathbb{C}^{K\times1}$, $K$ -- число адаптивных параметров модели.

Рассмотрим требования к целевой функции (метрике) алгоритмов компенсации. Метрика величины отклонения выхода модели помехи от вектора отсчетов самой помехи должна быть вещественнозначной и неотрицательной для реализации процедуры поиска минимума данной метрики. Кроме того, желательно задать целевую функцию квадратичной и выпуклой для применения эффективных методов адаптации \cite{convex_opt}.

Такой вещественной, неотрицательной и квадратичной метрикой является средний квадрат ошибки (MSE - Mean Square Error). Физический смысл среднего квадрата ошибки -- энергия отклонения выхода нелинейной модели от помехи, измеренной на приёмнике. При блочной обработке сигнала MSE имеет вид:
\begin{equation}
	J=||\bit{e}||_2^2=\textbf{\textit{e}}^H\textbf{\textit{e}}.
	\label{mse_block}
\end{equation}
Помимо MSE на практике применяется метрика - нормированный средний квадрат ошибки (NMSE - Normalized Mean Square Error):
\begin{equation}
	J=\frac{||\bit{e}||_2^2}{||\bit{d}||_2^2}=\frac{\textbf{\textit{e}}^H\textbf{\textit{e}}}{\textbf{\textit{d}}^H\textbf{\textit{d}}}.
	\label{nmse_block}
\end{equation}
В данном случае энергия ошибки нормируется к энергии сигнала приёмника для того чтобы оценить уровень ошибки независимо от динамического диапазона сигнала на входе применика. 

Реальные сигналы, используемые в системах связи \cite{sklyar_dig_telecom} являются мощностными, то есть обладают бесконечной энергией на бесконечном промежутке времени , однако в уравнениях \eqref{mse_block}, \eqref{nmse_block} фигурирует энергия ошибки и сигнала помехи, рассчитанная на конечной длине блока.

\section{Методы первого порядка для адаптации моделей нелинейных искажений}

Методы первого порядка \cite{polyak_optimiz} занимают центральное место в задачах адаптации нелинейных моделей, применяемых для компенсации искажений в трактах телекоммуникационных устройств. Их ключевое преимущество заключается в низкой вычислительной сложности и хорошей масштабируемости, что делает их особенно привлекательными для аппаратной реализации в системах с ограниченными ресурсами.

В отличие от более сложных второпорядковых алгоритмов, методы первого порядка — такие как стохастический градиентный спуск и его модификации — опираются лишь на локальную информацию о градиенте функции ошибки, обеспечивая устойчивую и быструю адаптацию даже при неполном знании статистики сигнала. Благодаря этому они хорошо подходят для работы в реальном времени и могут эффективно функционировать при изменяющихся условиях канала или параметров нелинейности.

Кроме того, градиентные методы допускают реализацию в фиксированной точке, что позволяет использовать простые арифметические блоки и снижает энергопотребление. Такая особенность делает их оптимальным выбором для встроенных и энергоэффективных систем, где важно обеспечить баланс между скоростью сходимости и аппаратной сложностью. В контексте нелинейной компенсации это обеспечивает возможность динамического обновления коэффициентов модели с минимальными затратами ресурсов при сохранении высокой точности восстановления сигнала.

\subsection{Метод градиентного спуска}

Градиентный спуск для случая нелинейной модели с вещественными параметрами представляет собой следующий итеративный алгоритм:
\begin{equation}
	\bit{z}_{k+1}=\bit{z}_{k}-\mu (D_{\bit{z}_k}J(\bit{z}_{k}))^T, \bit{z}_k\in\mathbb{R}^{K\times1},
	\label{grad_descent}
\end{equation}
\begin{equation}
	\begin{Vmatrix}
		D_{\bit{z}^*}J(\bit{z}_{k})-
		D_{\bit{z}^*}J(\bit{z}_{k+1})
	\end{Vmatrix}_2<
	L\begin{Vmatrix}
		D_{\bit{z}^*}J(\bit{z}_{k})-D_{\bit{z}^*}J(\bit{z}_{k+1})
	\end{Vmatrix}_2, \ \ 
	\mu\leqslant L,
	\label{lipshic_cond}
\end{equation}
где $D_{\bit{z}}J\in\mathbb{R}^{1\times K}$ -- оператор вычисления производной функции $J$ по адаптивным параметрам $\bit{z}$. Условие Липшица \cite{polyak_optimiz} задаёт ограничение на скорость поиска минимума целевой функции. Для ускорения алгоритма достаточно увеличить шаг $\mu$, однако при слишком больших $\mu$ - алгоритм расходится. Критическое значение, при котором наступает стагнация алгоритма -- $\mu_{кр}=\frac{L}{2}$.

Ввиду того, что в устройствах связи работают с комплекснозначными сигналами, параметры моделей нелинейной обработки сигналов также выбирают комплекснозначными. Направление возрастания вещественной функции комплексного переменного определяется производной вещественной функции по комплексно-сопряженным параметрам~\cite{wirt_deriv_book}. В связи с этим, метод градиентного спуска для моделей с комплексными параметрами строится следующим образом:
\begin{equation}
	\bit{z}_{k+1}=\bit{z}_{k}-\mu (D_{\bit{z}_k^*}J(\bit{z}_{k}))^T, \bit{z}_k\in\mathbb{C}^{K\times1},
	\label{grad_descent_сomplex}
\end{equation}
где $D_{\bit{z}_k^*}J\in\mathbb{C}^{1\times K}$ -- оператор вычисления производной вещественной функции $J$ по адаптивным комплексно-сопряженным параметрам $\bit{z}^*$.

Получим разностное уравнение метода градиентного спуска оптимизации вещественной скалярной функции потерь, зависящей от комплексных векторых параметров. Для этого разложим функцию потерь $J$ в окрестности нуля в ряд Тейлора до членов первого порядка малости:
\begin{equation}
	dJ=D_{\bit{z}}Jd\bit{z}+D_{\bit{z}^*}Jd\bit{z}^*\in\mathbb{R},
	\label{first_diff}
\end{equation}
где $D_{\bit{z}}J\in\mathbb{C}^{1\times K}$, $D_{\bit{z}^*}J\in\mathbb{C}^{1\times K}$, $d\bit{z}\in\mathbb{C}^{K\times 1}$, $d\bit{z}^*\in\mathbb{C}^{K\times 1}$.
Разложим выход нелинейной модели в Ряд Тейлора в окрестности нуля до членов первого порядка малости~\cite{wirt_deriv_book}:
\begin{equation}
	d\bit{y}=D_{\bit{z}}\bit{y}d\bit{z}+D_{\bit{z}^*}\bit{y}d\bit{z}^*\in\mathbb{C}^{N\times 1},
	\label{first_diff_model}
\end{equation}
где $D_{\bit{z}}\bit{y}$, $D_{\bit{z}^*}\bit{y}\in\mathbb{C}^{N\times K}$ -- матрицы Якоби, производные выхода модели по прямым и комплексно-сопряженным параметрам модели.

Распишем дифференциал среднего квадрата ошибки, введенного в~\eqref{mse_block} и подставим разлоежние выхода нелинейной модели~$\bit{y}$ до членов первого порядка малости~\eqref{first_diff_model}:
\begin{align}
	dJ&=d(\bit{e}^H\bit{e})=d\bit{e}^H\bit{e}+\bit{e}^Hd\bit{e}=-\bit{e}^Td\bit{y}^*-\bit{e}^Hd\bit{y}= \nonumber\\
	&=-\bit{e}^TD_{\bit{z}}\bit{y}^*d\bit{z}-\bit{e}^TD_{\bit{z}^*}\bit{y}^*d\bit{z}^*-\bit{e}^HD_{\bit{z}}\bit{y}d\bit{z}-\bit{e}^HD_{\bit{z}^*}\bit{y}d\bit{z}^*= \nonumber \\
	&=(-\bit{e}^TD_{\bit{z}}\bit{y}^*-\bit{e}^HD_{\bit{z}}\bit{y})d\bit{z}+(-\bit{e}^TD_{\bit{z}^*}\bit{y}^*-\bit{e}^HD_{\bit{z}^*}\bit{y})d\bit{z}^*.
	\label{first_diff_model_through_y}
\end{align}
Сравнивая выражения перед дифференциалом $\bit{z}^*$ в~\eqref{first_diff} и~\eqref{first_diff_model_through_y}, получаем выражение производной функции потерь по комплексно-сопряженным параметрам~$\bit{z}^*$:
\begin{equation}
	(D_{\bit{z}^*}J)^T=(-\bit{e}^TD_{\bit{z}^*}\bit{y}^*-\bit{e}^HD_{\bit{z}^*}\bit{y})^T=-(D_{\bit{z}}\bit{y})^H\bit{e}-(D_{\bit{z}^*}\bit{y})^T\bit{e}^*\in\mathbb{C}^{K\times 1}.
	\label{loss_deriv_nonholomorphic}
\end{equation}
Таким образом, подставляя~\eqref{loss_deriv_nonholomorphic} в~\eqref{grad_descent_сomplex} получаем разностное уравнение градиентного спуска для функции потерь MSE с вектором ошибки общего вида, то есть зависящим от прямых и сопряженных параметров $\bit{e}=\bit{e}(\bit{x}, \bit{z}, \bit{z}^*)$:
\begin{equation}
	\bit{z}_{k+1}=\bit{z}_{k}+\mu (D_{\bit{z}}\bit{y})^H\bit{e}+\mu(D_{\bit{z}^*}\bit{y})^T\bit{e}^*\in\mathbb{C}^{K\times1}
	\label{grad_descent_сomplex_mse_nonholomorphic}
\end{equation}

Пусть вектор отклонения выхода модели от значений помехи $\bit{e}(\bit{x}, \bit{z})\in\mathbb{C}^{N\times 1}$~--- голоморфная функция относительно \bit{z}, то есть не зависит от вектора комплексно-сопряженных араметров $\bit{z}^*$. В этом случае Якобиан выхода модели от сопряженных параметров равен нулю:
\begin{equation}
	D_{\bit{z}^*}\bit{y}=\bit{0}.
	\label{condition_holomorphic}
\end{equation}
Тогда из~\eqref{grad_descent_сomplex_mse_nonholomorphic} разностное уравнение метода градиентного спуска может быть представлено как:
\begin{equation}
	\bit{z}_{k+1}=\bit{z}_k+\mu(D_{\bit{z}}\bit{y})^H\bit{e}.
	\label{grad_descent_сomplex_mse_holomorphic}
\end{equation}

В нелинейной адаптивной обработке сигналов условие~\eqref{condition_holomorphic} используется при построении нелинейных моделей, поскольку в этом случае снижается вычислительная сложность пересчета параметров адаптивных моделей, снижается требования по памяти, энергопотребление, а также занимаемая площадь на кристалле.

\subsection{Модификации метода градиентного спуска}

Классический метод градиентного спуска~\eqref{grad_descent_сomplex} является эффективным в случае с гладкими выпуклыми функциями потерь, однако в случае сильно искривлённого ландшафта или шумных градиентов он может сходиться медленно. Для ускорения сходимости и сглаживания колебаний используется метод тяжёлого шара (momentum), предложенный Поляком~\cite{polyak1964}. Основная идея заключается во введении вектора скорости, аккумулирующего информацию о предыдущих шагах:

\begin{equation}
	\begin{cases}
		\bit{m}_{k+1} = \beta\,\bit{m}_k + \mu\,(D_{\bit{z}^*}J(\bit{z}_k))^T,\\[4pt]
		\bit{z}_{k+1} = \bit{z}_k - \bit{m}_{k+1},
	\end{cases}
	\label{eq:momentum}
\end{equation}
где \(0 \le \beta < 1\) — коэффициент инерции. Такой подход позволяет снизить осцилляции и ускорить движение вдоль пологих направлений, однако градиент вычисляется в текущей точке, что может приводить к избыточным колебаниям.

Для устранения этого недостатка была предложена модификация Нестерова , известная как \textit{ускоренный градиент Нестерова}~\cite{nesterov1983method} (Nesterov accelerated gradient, NAG). В этом подходе градиент вычисляется в «прогнозируемом» положении, что стабилизирует алгоритм адаптации:

\begin{equation}
	\begin{cases}
		\bit{m}_{k+1} = \beta\,\bit{m}_k + \mu\,(D_{\bit{z}^*}J(\bit{z}_k - \beta\,\bit{v}_k))^T,\\[4pt]
		\bit{z}_{k+1} = \bit{z}_k - \bit{m}_{k+1}.
	\end{cases}
	\label{eq:nesterov}
\end{equation}

Таким образом, данный метод корректирует направление движения до обновления параметров, что обеспечивает более устойчивую и быструю сходимость для выпуклых задач.

Дальнейшее развитие концепции инерционного градиентного спуска привело к созданию алгоритма \textit{Adam} (Adaptive Moment Estimation) \cite{kingma2014adam}, который сочетает механизмы накопления импульса градиента и адаптивной нормализации шага по каждой координате параметров. В отличие от классических схем с фиксированной скоростью обучения, Adam автоматически масштабирует величину шага в зависимости от первой и второй статистических моментов градиента, что обеспечивает баланс между скоростью сходимости и устойчивостью обновлений. Такая адаптивность позволяет алгоритму поддерживать высокую скорость обучения на ранних этапах и эффективно выходить из окрестностей локальных минимумов, что делает его одним из наиболее универсальных и применимых методов стохастической оптимизации:

\begin{equation}
	\begin{cases}
		\bit{m}_{k+1} = \beta_1\,\bit{m}_k + (1-\beta_1)\,(D_{\bit{z}^*}J(\bit{z}_k))^T,\\[4pt]
		\bit{v}_{k+1} = \beta_2\,\bit{v}_k + (1-\beta_2)\,\bigl|(D_{\bit{z}^*}J(\bit{z}_k))^T\bigr|^2,\\[4pt]
		\hat{\bit{m}}_{k+1} = \dfrac{\bit{m}_{k+1}}{1 - \beta_1^{k+1}},\qquad
		\hat{\bit{v}}_{k+1} = \dfrac{\bit{v}_{k+1}}{1 - \beta_2^{k+1}},\\[6pt]
		\bit{z}_{k+1} = \bit{z}_k - \mu\,\dfrac{\hat{\bit{m}}_{k+1}}{\sqrt{\hat{\bit{v}}_{k+1}} + \varepsilon},
	\end{cases}
	\label{eq:adam}
\end{equation}
где \(\beta_1 \approx 0.9\), \(\beta_2 \approx 0.999\), \(\varepsilon \approx 10^{-8}\).  
Adam объединяет преимущества момента (инерция) и адаптивных методов (нормализация шага по координатам), что обеспечивает стабильную сходимость даже при шумных или нестационарных градиентах.

Эволюция этих подходов отражает постепенное улучшение базового метода: от простого градиентного шага~\eqref{grad_descent_сomplex} к инерционному ускорению~\eqref{eq:momentum}, затем к прогнозирующему ускорению Нестерова~\eqref{eq:nesterov}, и, наконец, к адаптивно-инерционной схеме~\eqref{eq:adam}.